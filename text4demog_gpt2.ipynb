{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text4demog_gpt2.ipynb",
      "provenance": [],
      "mount_file_id": "1d1QONXuMsbadRC3ni5eR63tF7bk_kLq3",
      "authorship_tag": "ABX9TyOl/tzha+Snrq1zUvTwTgL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikennedy240/text4demog/blob/master/text4demog_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfmPyT7TjCzM"
      },
      "source": [
        "# Text4Demog Text Generation Using Transformers and GPT-2\n",
        "The `transformers` library, or [huggingface](https://huggingface.co/landing/inference-api/startups?utm_source=Google&utm_medium=Search&utm_campaign=Transformers+10x+Faster&utm_id=12055067954&gclid=CjwKCAjwzaSLBhBJEiwAJSRokg1r6FSo8X9OiDO2Gey41WMxEO8fNj8Odw2Twb9NmKBkrWFLnjAVtRoCGYEQAvD_BwE) is an easy to use deep learning library. It has tools for image and text data, and has many pre-trained models that you can download and fine-tune for your task. This short demo is based on work I did to produce a survey experiment that used computer-generated texts as treatments. It's designed to run in google colab, but you could run it fairly easily on any jupyter kernel with a gpu (and maybe less easily on a kernel with no gpu). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_7rBfpPWxQ"
      },
      "source": [
        "\n",
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "    Navigate to Edit→Notebook Settings\n",
        "    select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBMrkvzyPZwh",
        "outputId": "cc32c31a-dc61-46a7-efdb-239fbbc62cdf"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqtYbg0sfOPp"
      },
      "source": [
        "## Make sure you have data access\n",
        "Great! Next, I reccomend connecting to your google drive. As long as the shared google drive folder `text4demog` is in your main gdrive folder, the rest of the code should run as written. Alternatively, you could upload the data to google colab, but then just alter the `data_path` value below.\n",
        "\n",
        "To mount your gogole drive, go to the 'files' pane on the left, and click the folder with the drive logo (far right). Then follow the instructions.\n",
        "\n",
        "You can also copy the following code into a chunck and run it:\n",
        "``` \n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(‘/content/gdrive’)\n",
        "```\n",
        "\n",
        "Then you can test to see if you have access to the `text4demog` folder by running `os.listdir('/content/drive/MyDrive/text4demog')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTZLoeC2PYVc"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "!pip install transformers\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaeAxpHfgSV"
      },
      "source": [
        "If you haven't connected to your google drive, you'll need to upload the data and alter the datapath in the next chunk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWI3LZZ2fdq0"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/text4demog/cl_text4demog.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_AkKWR0Pqqc",
        "outputId": "1686a610-0b97-46c6-ede1-a4987a38bf33"
      },
      "source": [
        "df = pd.read_csv(data_path)\n",
        "# slice samples from each category and one with mixed texts\n",
        "hight50 = df[df.nh_text.str.contains('<hight50>')].sample(100).nh_text.to_list()\n",
        "lowt50 = df[df.nh_text.str.contains('<lowt50>')].sample(100).nh_text.to_list()\n",
        "mixed = df.sample(300).nh_text.to_list()\n",
        "# Identify which slice to use in analysis below\n",
        "text_list = hight50\n",
        "text_list[:5]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<hight50> Ballantyne Townhouse\\n\\n\\r 2 Bed Room 2 1/2 Bath Townhouse in great location, close to everything. Has gas fireplace,\\r sunken den, wet bar, eat in kitchen, large bed rooms, lots of closet space, private backyard. pool\\r and tennis court, 2 private reserved parking spaces.',\n",
              " \"<hight50> FOREST PARK NEWLY REHABBED BEAUTIFUL 2 BDR available NOW!\\n\\n\\r This newly renovated apartment is a must see! Location location location! One block from the Blue line and 290! Four blocks from the green line! No worries about parking because it's included along with heat and water! This 2 bedroom apartment has walk-in closets, a large living room and dining room with a beautiful fireplace. Granite and new stainless steel appliances. EXTRA PERKS include PRIVATE ENCLOSED PORCH with lots of windows and backyard! Everything you need!  Pets considered. $1500.00 a month. Don't let this one get away! Available 6/1! CALL PREFERRED John\\r show contact info\\r  click to show contact info\",\n",
              " \"<hight50> 2BD/1BA Available August 1st in Prime Wicker Park Location!\\n\\n Vintage but well-maintained 2-bedroom available on August 1st in PRIME Wicker Park location. Apartment is a 3-minute walk to the Division Blue Line CTA stop. Gorgeous hardwood floors throughout. Ample natural light. Large kitchen with beautiful hutch included for added storage. Larger bedroom can accommodate a queen-sized bed while the smaller bedroom can accommodate a full-sized bed. Laundry in the building. Storage unit included. Bike storage available. Permit parking only. Live steps to the restaurants, bars, and shops on Division Street near Ashland! This one won't last long! Email or text for a showing! Melissa Mancini Fulton Grace Realty\",\n",
              " '<hight50> Renovated 2 Bedroom W/  Fireplace!\\n\\n\\r Call Now -\\r show contact info\\r  click to show contact info\\r This is a 2 Bedroom, 1 Bath, approximately 1018 Sq. Ft.\\r What Makes Our Community Unique...\\r Location, location, location. Our community is located half way between downtown Vinings and Smyrna, with easy access to 75 and 285. Downtown Atlanta is minutes away, and the new Braves Stadium is around the corner.\\r Features:\\r Sunrooms, Sunwalls, Bay Windows, Vaulted Ceilings, Grand Living Room and Dining Room, Fully-Equipped Kitchens\\r Community Amenities:\\r Bark Park with Dog Run, Controlled Access Gates, Gas Grills, Green Certified, Executive Business and Conference Center, Fully-Equipped Cardio Theater, Two Lighted Outdoor Tennis Courts, 24 Hour Laundry Center, Two Car Care Centers, Two Sparkling Swimming Pools with Waterscapes, Detached Private Garages Available, Gazebo with Picnic Area\\r Pet Policy:\\r Pets - Max 2 allowed, Max weight 95 lb each, One time Fee $400.00, Rent $15.00\\r Comments: Dogs & Cats\\r Restrictions: Greystar Standard\\r Use the link below for more details:\\r http://cl.greystar.com/25g5qd\\r Equal Housing Opportunity\\r 9ejDiKBNJt4',\n",
              " \"<hight50> #247 Fall in Love with your New home at Harbor Terrace!!\\n\\n With breathtaking views and a secluded community, located in Tacoma's historical Stadium District Harbor Terrace is the destination for you! Here at Harbor Terrace we offer one and two bedroom large apartment homes with gorgeous views of Mt. Rainier, Commencement Bay and Tacoma's sparkling city nights. We are located near many local shops, eateries, parks, recreations and historical landmarks which are only minutes away. Harbor Terrace is the perfect location to relax on your balcony and enjoy the tranquility and beauty of the majestic mountains and calming Commencement Bay. Free parking in Garage!!! $100 Boiler Fee Call us now to schedule your VIP tour!! Office Hours: Monday - Friday: 9am - 6pm Saturday:  9am - 5pm Sunday: Closed Office location: The Orion 29 Saint Helens Ave Tacoma, WA 98402\"]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjW1YhoAQbyy",
        "outputId": "45f0bc28-bd89-43b8-ac62-02bd75c96fd0"
      },
      "source": [
        "# Load the GPT tokenizer.\n",
        "special_tokens = ['<hight50>','<lowt50>','<mediumt50>']\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXocRYDIR-aC",
        "outputId": "002e1c50-26b5-41f5-f35b-c77c4a4051fc"
      },
      "source": [
        "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
            "The beginning of sequence token <|startoftext|> token has the id 50257\n",
            "The end of sequence token <|endoftext|> has the id 50256\n",
            "The padding token <|pad|> has the id 50258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzJe0LaFSCBj"
      },
      "source": [
        "batch_size = 2"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-VgcMnsSFig"
      },
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx] "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IanW3RdmSItX"
      },
      "source": [
        "dataset = GPT2Dataset(text_list, tokenizer, max_length=768)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkBdBAsfSLb2",
        "outputId": "0620746c-18eb-44d0-efe2-80878220773c"
      },
      "source": [
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  900 training samples\n",
            "  100 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ff0tLnSUhN"
      },
      "source": [
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_0ctTnmSddq",
        "outputId": "53d2d3bd-cddf-44b1-bc67-188560f89dfe"
      },
      "source": [
        "# I'm not really doing anything with the config buheret\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50262, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iCDOiqoSjAu"
      },
      "source": [
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLU2RjZ4SmPN"
      },
      "source": [
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "# some parameters I cooked up that work reasonably well\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 100\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGCZVXB9SxO8",
        "outputId": "8b1cc8d8-bba6-4d06-b773-4c06ff53feff"
      },
      "source": [
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "today_str = datetime.datetime.now()\n",
        "today_str = today_str.strftime('%Y_%m_%d')\n",
        "\n",
        "output_dir = input(f'''\n",
        "Set a directory to save the model (d)efault: \n",
        "/content/gpt2_{today_str}\\n\n",
        "But use \"/content/drive/MyDrive/<yourmodelname>\" to save to google drive ''')\n",
        "\n",
        "if output_dir == 'd':\n",
        "  ouptut_dir = f'gpt2_{today_str}'\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the model name, or use (d)efault: gpt2_2021_10_15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gbYeJHXTRz0"
      },
      "source": [
        "total_t0 = time.time()\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids, \n",
        "#                            token_type_ids=None, \n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "print(df_stats)\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7JUudldg5Mm"
      },
      "source": [
        "## Model Evaluation\n",
        "Now that we've trained the model, we want to see what kinds of texts it can produce. You have the option here of using a model that's you've just trained or to load a model that you've trained previously. There're already some models you can test in the `text4demog/models` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVb93X7iT7dX",
        "outputId": "9bcb7ef5-9ae7-4bbc-8e9d-78b63527b0ba"
      },
      "source": [
        "if 'model' in globals():\n",
        "  load_model = input(\"You have a model loaded already, would you like to load a different one? (Y/n)\\n\")\n",
        "\n",
        "if 'model' not in globals() or load_model in ['y','Y']:\n",
        "  model_path = input(\"Input the directory that contains your gpt-2 model:\\n\")\n",
        "\n",
        "  #save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "  # They can then be reloaded using `from_pretrained()`\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "  # Good practice: save your training arguments together with the trained model\n",
        "  # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.cuda()\n",
        "  model = model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = input(\"Generating Text. Enter starting tokens or use (d)efault:\\n\")\n",
        "if prompt =='d':\n",
        "  prompt = \"<|startoftext|>\"\n",
        "\n",
        "n = input(\"How many texts should we generate?\")\n",
        "n = int(n)\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                              generated, \n",
        "                              #bos_token_id=random.randint(1,30000),\n",
        "                              do_sample=True,   \n",
        "                              top_k=50, \n",
        "                              max_length = 300,\n",
        "                              top_p=0.95, \n",
        "                              num_return_sequences=n\n",
        "                              )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have a model loaded already, would you like to load a different one? (Y/n)n\n",
            "Generating Text. Enter starting tokens or use (d)efault:\n",
            "d\n",
            "How many texts should we generate?3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50257]], device='cuda:0')\n",
            "0: Newly Renovated, Stainless Steel Appliances, 2 Bedroom/1 Bathroom, Parking Included!\n",
            "\n",
            "\n",
            "PROPERTY INFO\n",
            "ID: 314471489\n",
            "Rent: $2,998 / Month\n",
            "Beds: 2\n",
            "Bath: 1\n",
            "Available Date: 10/31/2020\n",
            "Pet: Cat Ok\n",
            "Parking:: Available!\n",
            "AVAILABLE NOW\n",
            "2 Bedroom Apartment for $2,998. Includes a Washer/Dryer in unit\n",
            "2nd Bedrooms\n",
            "Living Room\n",
            "Dining Room\n",
            "Dining Room\n",
            "Kitchen w/ SS Appliances w/ SS Appliances\n",
            "Hardwood Floors\n",
            "Pets Allowed\n",
            "Carpet flooring\n",
            "Heat and Hot Water Included in the Rent\n",
            "Grocery Stores Near by\n",
            "Great Price! Available 9/1!\n",
            "\n",
            "\n",
            "1: 2 BED 1 BATH - GREAT LOCATION\n",
            "\n",
            "\n",
            "This well located 2 bed/1 bath in prime Santa Monica location.\n",
            "- Conveniently located\n",
            "- 10 minute walk to beach\n",
            "- Conveniently located just outside the door\n",
            "- Great community for beach hopping! - Steps to everything!!\n",
            "- No fee\n",
            "- 1 parking space included\n",
            "- Tenant pays for water\n",
            "- Non-smoking included\n",
            "- Cats OK (breed restrictions apply)\n",
            "- No smoking\n",
            "- Great water views!\n",
            "- Walking distance to restaurants and park\n",
            "- Short walk to beach\n",
            "- Easy to access I-20 and I-520\n",
            "- 10 minute walk to Downtown LA.\n",
            "\n",
            "\n",
            "2: 2 Bedroom 1 Bath\n",
            "\n",
            "\n",
            "PROPERTY INFO\n",
            "ID: 11457726\n",
            "Rent: $2300 / Month\n",
            "Beds: 2\n",
            "Bath: 1\n",
            "Available Date: 08/01/2020\n",
            "Pet: Dog\n",
            "Parking:: - NO BROKERS FEE\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMev_ke9gu0V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}