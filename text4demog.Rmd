---
title: "text4demog"
author: "Ian Kennedy"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stm)
library(tm)
```


## Example 0: Craigslist Ads

We'll use a sample of 2000 craigslist listings for today.   
```{r results = 'asis'}
set.seed(1024)
# if this doesn't load, make sure you've either opened the rproject folder, or set your working directory to the text4demog repo
cl_sample <- read_csv('data/seattle_sample_demog.csv', col_types = cols(.default = 'c')) %>% mutate(cleanRent = as.numeric(cleanRent))%>% drop_na(cleanRent)
```
Raw text looks like this:   
QR Code Link to This Post
Contact info:
Pinewood Village Apartment Homes |
show contact info
|
show contact info
Awesome View in this Home & the Perfect Location in Bellevue!
16 149th Ave NE Apt C, Bellevue, WA 98007
$1775/mo
KEY FEATURES
Year Built:
1983
Sq Footage:
Approx. 960 sq ft.
Bedrooms:
2 Beds
Bathrooms:
1 Baths
Parking:
1 Carport | Guest parking
Lease Duration:
6, 9, 12 Months
Deposit:
$400

## Example 0: Examining Keyword Prevalence
Commonly we start with some words that we think might be important. This vignette examines the prevalence of certain keywords in the sample as a whole, and then in relation to a covariate.

```{r}
# let's use tidy text to look at overall prevalence
cl_sample %>% unnest_tokens(word, cleanText) %>% # tokenize, remove punctuation
  filter(!nchar(word) < 3) %>% # remove short words
  anti_join(stop_words, by = "word") %>% # remove stop words 
  count(word) %>% arrange(desc(n))

# then pick some keywords

keywords <- c("rent", "parking",'popular', 'secluded')

# and use just the first one to start
keyword <- keywords[1]

cl_sample %>% mutate(fake_date = round(parse_number(postID)/1e7)) %>%
  mutate(rent = str_detect(cleanText, 'rent'),
         popular = str_detect(cleanText, 'popular'),
         secluded = str_detect(cleanText, 'secluded')) %>% 
  group_by(fake_date) %>% 
  summarise(rent = mean(rent), popular = mean(popular), secluded = mean(secluded)) %>%
  gather(keyword, prevalance, rent, popular, secluded) %>%
  ggplot(aes(fake_date, prevalance, color = keyword, group = keyword))+
  geom_point()+
  facet_wrap(~keyword)

```


## Example 1: Words that Make Units Expensive
An easy sort of analysis is one that works and the word level and looks at some association with a covariate. The tidytext library in R is good for this. In pythong I use the NLTK along with SKlearn.

```{r}
# we want to clean and tokenize, that's what this function does
prep_texts_only <- function(df){
  # remove annoying stuff like urls
df <- df %>% mutate(
  text = str_replace_all(text, '(http)?(www)?\\S*(\\.com|\\.net|\\.gov|\\.be|\\.org)\\S*', ''),
  text = str_replace_all(text,"(\\w+)\\W|/(\\w+)","\\1 \\2"),
  text = str_remove_all(text, '\\d'),
  text = str_remove(text, 'https|t.co')) # separate words joined by non-word characters

df_toke <- df %>% 
  unnest_tokens(word,text, strip_punct = TRUE) %>% # tokenize, remove punctuation
  filter(!nchar(word) < 3) %>% # remove short words
  anti_join(stop_words, by = "word") # remove stop words
# go wide with dummies
df_toke <- inner_join(df_toke, df_toke %>% count(word) %>% filter(n>2), by = 'word') %>% 
  select(-n) %>% 
  mutate(present = 1) %>% 
  distinct(postID, word, .keep_all = TRUE) %>% 
  spread(word, present, fill = 0)
if(exists('df_toke$class_label')){
  df_toke <- df_toke %>% select(postID, class_label, everything()) 
} else {
  df_toke <- df_toke %>% select(postID, everything()) 
}
return(df_toke)
}
# this will give us a very wide table
cl_sample_toke <- cl_sample %>% mutate(text = str_c(listingTitle, listingText)) %>% prep_texts_only()
# now we'll fit a model looking at words associated with higher white proportion
m0 <- lm(paste0('cleanRent ~ `', paste0(names(cl_sample_toke)[27:ncol(cl_sample_toke)], collapse = '`+`'), '`'), data = cl_sample_toke %>% filter(cleanRent<4000, cleanRent>300))
sort(m0$coefficients, decreasing = TRUE)[1:20]
```


## Example 2: STM to model differences of theme with covariate

At the word level, this analysis doesn't tell us that much. It might be more worthwhile to investigate the document level. 

I like to use STM, a topic modeling method, to do this.

```{r}

## PROCESS TRAINING DATA
temp <- textProcessor(documents = cl_sample$listingText, meta=cl_sample, onlycharacter = TRUE) 
out <- prepDocuments(temp$documents, temp$vocab, meta = temp$meta)

```

```{r}
stm_1 <- stm(out$documents, 
                     out$vocab, 
                     K = 12,
                     prevalence = ~ cleanRent,
                     data = out$meta,
                     seed = 24)
labelTopics(stm_1)
```

```{r}
effect <- estimateEffect(1:12 ~ 1+ cleanRent, stm_1, out$meta)
plot.estimateEffect(effect, 'cleanRent', stm_1, method = 'difference', cov.value1 = quantile(out$meta$cleanRent, .25), cov.value2 = quantile(out$meta$cleanRent, .75))
```

---
I think that visualization is ugly
```{r}
sum_effect <- summary(effect)
coef_sum <- tibble()
for(i in 1:length(sum_effect$tables)){
  coef_sum <- bind_rows(coef_sum, sum_effect$tables[[i]][2,])
}
ggplot(coef_sum %>% mutate(topic = factor(paste("topic", 1:12))), aes(x = Estimate, y = reorder(topic, -Estimate)))+
  geom_point()+
  geom_errorbarh(aes(xmin = Estimate-(1.96*`Std. Error`),xmax = Estimate+(1.96*`Std. Error`)))+
  geom_vline(xintercept = 0, color = "red")+
  theme_minimal()+
  theme(axis.title.y = element_blank())
```

---
That sends us back to look at the topics again:
```{r}
topics_of_interest = c(10,7)
labelTopics(stm_1, topics_of_interest)
```

---
```{r}
findThoughts(stm_1,texts = out$meta$listingText, topics = topics_of_interest[1], n =1)
```

```{r}
findThoughts(stm_1,texts = out$meta$listingText, topics = topics_of_interest[2], n =1)
```



